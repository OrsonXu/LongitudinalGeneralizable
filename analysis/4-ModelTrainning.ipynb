{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "\n",
    "NAFILL = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "folder_path = config[\"data_root_path\"]\n",
    "focus_path = folder_path + \"Selection_Group/\" + config[\"label_focus\"] + \"/\"\n",
    "eps_long_to_short = config[\"eps_long_to_short\"]\n",
    "epis = config[\"epis\"]\n",
    "wks = config[\"wks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(focus_path + \"raw_data_pool.pkl\", \"rb\") as f:\n",
    "    raw_data_pool = pickle.load(f)\n",
    "df_phaseI_select = raw_data_pool[\"data_phaseI\"]\n",
    "df_phaseII_select = raw_data_pool[\"data_phaseII\"]\n",
    "df_label = raw_data_pool[\"labels\"]\n",
    "df_label = df_label.set_index(\"PID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + \"feature_dis_col_epis.json\",\"r\") as f:\n",
    "    feature_dis_col_epis = json.load(f)\n",
    "with open(folder_path + \"symbol_to_int_map_day.json\", \"r\") as f:\n",
    "    symbol_to_int_map_day = json.load(f)\n",
    "with open(folder_path + \"int_to_symbol_map_day.json\", \"r\") as f:\n",
    "    int_to_symbol_map_day = json.load(f)\n",
    "with open(folder_path + \"int_to_feature_map_day.json\", \"r\") as f:\n",
    "    int_to_feature_map_day = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(focus_path + \"top_features.json\", \"r\") as f:\n",
    "    top_features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID_list_grp1_phaseI = df_label[df_label[\"label_I\"]].index.tolist()\n",
    "PID_list_grp2_phaseI = df_label[~df_label[\"label_I\"]].index.tolist()\n",
    "PID_list_grp1_phaseII = df_label[df_label[\"label_II\"]].index.tolist()\n",
    "PID_list_grp2_phaseII = df_label[~df_label[\"label_II\"]].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for wk in wks:\n",
    "    for epi in epis:\n",
    "        count += len(top_features[wk][epi])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_unimodal_feature(df_data):\n",
    "    df_unimodal_asso = {}\n",
    "    for wk in wks:\n",
    "        df_unimodal_asso[wk] = {}\n",
    "        for epi in epis:\n",
    "            print(wk,epi)\n",
    "            ruleset1 = pd.read_csv(focus_path + \"SPMF_select/\" + \"asso_rule_straight_\" + wk + \"_\" + epi + \".csv\")\n",
    "            ruleset2 = pd.read_csv(focus_path + \"SPMF_select/\" + \"asso_rule_nostraight_\" + wk + \"_\" + epi + \".csv\")\n",
    "            ruleset = pd.concat([ruleset1, ruleset2])\n",
    "            ruleset.reset_index(drop = True, inplace = True)\n",
    "            ruleset[\"X\"] = ruleset[\"X\"].apply(lambda x : ast.literal_eval(x))\n",
    "            ruleset[\"Y\"] = ruleset[\"Y\"].apply(lambda x : ast.literal_eval(x))\n",
    "\n",
    "            df_unimodal_asso[wk][epi] = pd.DataFrame()\n",
    "            df_buf = deepcopy(df_data)\n",
    "\n",
    "            df_unimodal = df_buf[[\"PID\"] + top_features[wk][epi]]\n",
    "            df_unimodal_mean = df_unimodal.groupby(\"PID\").mean()\n",
    "            df_unimodal_mean.columns = [\"mean_\" + y for y in top_features[wk][epi]]\n",
    "            df_unimodal_std = df_unimodal.groupby(\"PID\").std()\n",
    "            df_unimodal_std.columns = [\"std_\" + y for y in top_features[wk][epi]]\n",
    "            df_unimodal_mean_std = df_unimodal_mean.merge(df_unimodal_std, how = \"outer\", left_index=True, right_index=True)\n",
    "#             df_unimodal_mean_std = deepcopy(df_unimodal_mean)\n",
    "            df_unimodal_asso[wk][epi] = deepcopy(df_unimodal_mean_std-df_unimodal_mean_std.mean())/df_unimodal_mean_std.std()\n",
    "#             if (df_unimodal_asso[wk][epi].shape[0] == 0):\n",
    "#                 df_unimodal_asso[wk][epi] = deepcopy(df_unimodal_mean_std)\n",
    "#             else:\n",
    "#                 df_unimodal_asso[wk][epi] = df_unimodal_asso[wk][epi].merge(df_unimodal_mean_std, how = \"outer\", left_index=True, right_index=True)\n",
    "#     #         df_unimodal_features[wk][epi] = deepcopy(df_unimodal_asso[wk][epi].columns)\n",
    "\n",
    "    df_unimodal_asso_combined = pd.DataFrame()\n",
    "    for wk in wks:\n",
    "        for epi in epis:\n",
    "            if (df_unimodal_asso_combined.shape[0] == 0):\n",
    "                df_unimodal_asso_combined = deepcopy(df_unimodal_asso[wk][epi])\n",
    "            else:\n",
    "                df_unimodal_asso_combined = df_unimodal_asso_combined.merge(df_unimodal_asso[wk][epi], how = \"outer\", left_index=True, right_index=True)\n",
    "    return df_unimodal_asso_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_contextual_feature_from_rule(df_data):\n",
    "    df_multimodal_asso = {}\n",
    "    for wk in wks:\n",
    "        df_multimodal_asso[wk] = {}\n",
    "        for epi in epis:\n",
    "            print(wk,epi)\n",
    "            ruleset1 = pd.read_csv(focus_path + \"SPMF_select/\" + \"asso_rule_straight_\" + wk + \"_\" + epi + \".csv\")\n",
    "            ruleset2 = pd.read_csv(focus_path + \"SPMF_select/\" + \"asso_rule_nostraight_\" + wk + \"_\" + epi + \".csv\")\n",
    "            ruleset = pd.concat([ruleset1, ruleset2])\n",
    "            ruleset.reset_index(drop = True, inplace = True)\n",
    "            ruleset[\"X\"] = ruleset[\"X\"].apply(lambda x : ast.literal_eval(x))\n",
    "            ruleset[\"Y\"] = ruleset[\"Y\"].apply(lambda x : ast.literal_eval(x))\n",
    "\n",
    "            df_multimodal_asso[wk][epi] = pd.DataFrame()\n",
    "            df_buf = deepcopy(df_data)\n",
    "\n",
    "            for rule_idx, rule in ruleset.iterrows():\n",
    "                index_flag = True\n",
    "                for x in rule[\"X\"]:\n",
    "                    dis_label_col = int_to_symbol_map_day[x][:-2] \n",
    "                    dis_label = int_to_symbol_map_day[x][-1]\n",
    "                    index_flag = index_flag & (df_buf[dis_label_col] == dis_label)\n",
    "                feature_columns = [int_to_feature_map_day[y] for y in rule[\"Y\"]]\n",
    "                feature_columns_name = [y + \"_\" + wk + \"_\" + epi + \"_rule\" + str(rule_idx) for y in feature_columns]\n",
    "\n",
    "                df_multimodal = df_buf[index_flag][[\"PID\"] + feature_columns]\n",
    "                df_multimodal_mean = df_multimodal.groupby(\"PID\").mean()\n",
    "                df_multimodal_mean.columns = [\"mean_\" + y for y in feature_columns_name]\n",
    "                df_multimodal_std = df_multimodal.groupby(\"PID\").std()\n",
    "                df_multimodal_std.columns = [\"std_\" + y for y in feature_columns_name]\n",
    "                df_multimodal_mean_std = df_multimodal_mean.merge(df_multimodal_std, how = \"outer\", left_index=True, right_index=True)\n",
    "    #             df_multimodal_mean_std = deepcopy(df_multimodal_mean)\n",
    "                df_multimodal_mean_std = (df_multimodal_mean_std-df_multimodal_mean_std.mean())/df_multimodal_mean_std.std()\n",
    "                if (df_multimodal_asso[wk][epi].shape[0] == 0):\n",
    "                    df_multimodal_asso[wk][epi] = deepcopy(df_multimodal_mean_std)\n",
    "                else:\n",
    "                    df_multimodal_asso[wk][epi] = df_multimodal_asso[wk][epi].merge(df_multimodal_mean_std, how = \"outer\", left_index=True, right_index=True)\n",
    "    #         df_multimodal_features[wk][epi] = deepcopy(df_multimodal_asso[wk][epi].columns)\n",
    "\n",
    "    df_multimodal_asso_combined = pd.DataFrame()\n",
    "    for wk in wks:\n",
    "        for epi in epis:\n",
    "            if (df_multimodal_asso_combined.shape[0] == 0):\n",
    "                df_multimodal_asso_combined = deepcopy(df_multimodal_asso[wk][epi])\n",
    "            else:\n",
    "                df_multimodal_asso_combined = df_multimodal_asso_combined.merge(df_multimodal_asso[wk][epi], how = \"outer\", left_index=True, right_index=True)\n",
    "    return df_multimodal_asso_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wkdy mo\n",
      "wkdy af\n",
      "wkdy ev\n",
      "wkdy ni\n",
      "wkend mo\n",
      "wkend af\n",
      "wkend ev\n",
      "wkend ni\n",
      "wkdy mo\n",
      "wkdy af\n",
      "wkdy ev\n",
      "wkdy ni\n",
      "wkend mo\n",
      "wkend af\n",
      "wkend ev\n",
      "wkend ni\n"
     ]
    }
   ],
   "source": [
    "df_unimodal_asso_combined_phaseI = obtain_unimodal_feature(df_phaseI_select)\n",
    "df_unimodal_asso_combined_phaseII = obtain_unimodal_feature(df_phaseII_select)\n",
    "df_unimodal_asso_combined_phaseI = df_unimodal_asso_combined_phaseI.loc[df_label.index]\n",
    "df_unimodal_asso_combined_phaseII = df_unimodal_asso_combined_phaseI.loc[df_label.index]\n",
    "df_unimodal_asso_combined_phaseI.to_csv(focus_path + \"X_unimodal_phaseI.csv\")\n",
    "df_unimodal_asso_combined_phaseII.to_csv(focus_path + \"X_unimodal_phaseII.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wkdy mo\n",
      "wkdy af\n",
      "wkdy ev\n",
      "wkdy ni\n",
      "wkend mo\n",
      "wkend af\n",
      "wkend ev\n",
      "wkend ni\n",
      "wkdy mo\n",
      "wkdy af\n",
      "wkdy ev\n",
      "wkdy ni\n",
      "wkend mo\n",
      "wkend af\n",
      "wkend ev\n",
      "wkend ni\n"
     ]
    }
   ],
   "source": [
    "df_contextual_asso_combined_phaseI = obtain_contextual_feature_from_rule(df_phaseI_select)\n",
    "df_contextual_asso_combined_phaseII = obtain_contextual_feature_from_rule(df_phaseII_select)\n",
    "df_contextual_asso_combined_phaseI = df_multimodal_asso_combined_phaseI.loc[df_label.index]\n",
    "df_contextual_asso_combined_phaseII = df_multimodal_asso_combined_phaseII.loc[df_label.index]\n",
    "df_contextual_asso_combined_phaseI.to_csv(focus_path + \"X_contextual_phaseI.csv\")\n",
    "df_contextual_asso_combined_phaseII.to_csv(focus_path + \"X_contextual_phaseII.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree  \n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "import sklearn\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(X,X_II,Y,Y_II,n_estimators = 10, max_depth = -1, max_leaf_nodes = -1):\n",
    "    if (max_depth == -1):\n",
    "        clf = ensemble.AdaBoostClassifier(n_estimators = n_estimators,\n",
    "                                          base_estimator = tree.DecisionTreeClassifier(\n",
    "#                                               max_depth= 5),\n",
    "                                              max_leaf_nodes = max_leaf_nodes),\n",
    "                                          learning_rate = 0.001)\n",
    "    else:\n",
    "        clf = ensemble.AdaBoostClassifier(n_estimators = n_estimators,\n",
    "                                          base_estimator = tree.DecisionTreeClassifier(\n",
    "                                              max_depth= max_depth),\n",
    "#                                               max_leaf_nodes = max_leaf_nodes),\n",
    "                                          learning_rate = 0.001)\n",
    "    results = {}\n",
    "    iter_round = 0\n",
    "\n",
    "    XX = deepcopy(X)\n",
    "\n",
    "    importance_index = range(len(XX.columns))\n",
    "    while (True):\n",
    "        iter_round += 1\n",
    "        print(XX.shape)\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "        y_pred_list = []\n",
    "        y_test_list = []\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "        for train_index, test_index in loo.split(XX):\n",
    "            X_train, X_test = XX.iloc[train_index,:], XX.iloc[test_index,:]\n",
    "            Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "            a = clf.fit(X_train,Y_train)\n",
    "            y_pred_list.append(a.predict(X_test))\n",
    "            y_test_list.append(Y_test)\n",
    "            train_acc.append(accuracy_score(a.predict(X_train),Y_train))\n",
    "            test_acc.append(accuracy_score(a.predict(X_test),Y_test))\n",
    "            \n",
    "        # retrain to obtain a subset feature\n",
    "        confusion_mx = confusion_matrix(y_test_list,y_pred_list)\n",
    "        results[iter_round] = {}\n",
    "        results[iter_round][\"train_acc\"] = np.mean(train_acc)\n",
    "        results[iter_round][\"test_acc\"] = np.mean(test_acc)\n",
    "        results[iter_round][\"confusion_matrix\"] = deepcopy(confusion_mx)\n",
    "\n",
    "        a = clf.fit(XX,Y)\n",
    "        importance = a.feature_importances_\n",
    "        importance_index_new = np.array([idx for idx, x in enumerate(importance) if x > 0])\n",
    "\n",
    "        if (len(importance_index_new) == len(importance_index)):\n",
    "            y_pred_phaseII = a.predict(X_II.fillna(NAFILL)[np.array(XX.columns.tolist())[importance_index_new]])\n",
    "            results[iter_round][\"test_acc_phaseII\"] = accuracy_score(Y_II,y_pred_phaseII)\n",
    "            results[iter_round][\"confusion_matrix_phaseII\"] = confusion_matrix(Y_II,y_pred_phaseII)\n",
    "            print(\"train acc phase1: \", results[iter_round][\"train_acc\"],\n",
    "                  \"test acc phase1: \", results[iter_round][\"test_acc\"],\n",
    "                  \"phase2: \", results[iter_round][\"test_acc_phaseII\"])\n",
    "            break\n",
    "        else:\n",
    "            results[iter_round][\"importance_feature\"] = deepcopy(np.array(XX.columns.tolist())[importance_index_new])\n",
    "            print(importance_index_new)\n",
    "            importance_index = deepcopy(importance_index_new)\n",
    "            XX = X[np.array(XX.columns.tolist())[importance_index]]\n",
    "            print(\"train acc \",np.mean(train_acc), \" test acc \", np.mean(test_acc))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unimodal Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phaseI_unimodal = pd.read_csv(focus_path + \"X_unimodal_phaseI.csv\")\n",
    "X_phaseI_unimodal = X_phaseI_unimodal.set_index(\"PID\")\n",
    "Y_phaseI = df_label[\"label_I\"]\n",
    "X_phaseII_unimodal = pd.read_csv(focus_path + \"X_unimodal_phaseII.csv\")\n",
    "X_phaseII_unimodal = X_phaseII_unimodal.set_index(\"PID\")\n",
    "Y_phaseII = df_label[\"label_II\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n",
      "5 2\n",
      "(68, 296)\n",
      "[ 30 126 142 159 174]\n",
      "train acc  0.883011413520632  test acc  0.47058823529411764\n",
      "(68, 5)\n",
      "train acc phase1:  0.8823529411764703 test acc phase1:  0.7794117647058824 phase2:  0.6617647058823529\n",
      "5 3\n",
      "(68, 296)\n",
      "[ 30  36  79  84  98 121 126 142 152 157 159 174 182 187 199]\n",
      "train acc  0.9664179104477614  test acc  0.5882352941176471\n",
      "(68, 15)\n",
      "[ 0  1  2  3  5  6  7  8 10 11 12]\n",
      "train acc  0.9554433713784022  test acc  0.8235294117647058\n",
      "(68, 11)\n",
      "train acc phase1:  0.9554433713784022 test acc phase1:  0.8382352941176471 phase2:  0.7352941176470589\n",
      "5 4\n",
      "(68, 296)\n",
      "[  6  11  29  30  40  98 126 142 149 152 159 169 174 182 259 273]\n",
      "train acc  0.9914398595258996  test acc  0.5735294117647058\n",
      "(68, 16)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "train acc  0.9855136084284456  test acc  0.8088235294117647\n",
      "(68, 15)\n",
      "[ 0  3  4  5  6  7  9 10 11 12 13 14]\n",
      "train acc  0.9855136084284456  test acc  0.8088235294117647\n",
      "(68, 12)\n",
      "[ 0  1  2  4  5  6  7  8  9 10 11]\n",
      "train acc  0.9855136084284456  test acc  0.8088235294117647\n",
      "(68, 11)\n",
      "train acc phase1:  0.9855136084284456 test acc phase1:  0.8235294117647058 phase2:  0.7647058823529411\n",
      "leaf\n",
      "5 3\n",
      "(68, 296)\n",
      "[ 30 126 142 182]\n",
      "train acc  0.8391132572431957  test acc  0.5441176470588235\n",
      "(68, 4)\n",
      "train acc phase1:  0.838235294117647 test acc phase1:  0.7941176470588235 phase2:  0.6176470588235294\n",
      "5 4\n",
      "(68, 296)\n",
      "[ 30 126 142 152 182]\n",
      "train acc  0.9036435469710273  test acc  0.5882352941176471\n",
      "(68, 5)\n",
      "train acc phase1:  0.8970588235294119 test acc phase1:  0.8382352941176471 phase2:  0.6764705882352942\n",
      "5 5\n",
      "(68, 296)\n",
      "[ 30 126 142 152 159 174 182]\n",
      "train acc  0.9512730465320457  test acc  0.5441176470588235\n",
      "(68, 7)\n",
      "train acc phase1:  0.940956979806848 test acc phase1:  0.8235294117647058 phase2:  0.7205882352941176\n",
      "5 6\n",
      "(68, 296)\n",
      "[ 30 126 142 152 159 169 174 182]\n",
      "train acc  0.9778314310798948  test acc  0.5588235294117647\n",
      "(68, 8)\n",
      "train acc phase1:  0.9705882352941181 test acc phase1:  0.8088235294117647 phase2:  0.75\n",
      "5 7\n",
      "(68, 296)\n",
      "[ 10  30  62  68 126 142 152 159 169 173 174 182]\n",
      "train acc  0.9885864793678664  test acc  0.5735294117647058\n",
      "(68, 12)\n",
      "[ 1  2  3  4  5  6  7  8 10 11]\n",
      "train acc  0.9846356453028972  test acc  0.7941176470588235\n",
      "(68, 10)\n",
      "train acc phase1:  0.9844161545215099 test acc phase1:  0.7794117647058824 phase2:  0.7352941176470589\n",
      "5 8\n",
      "(68, 296)\n",
      "[  3   5  22  30  60 126 139 142 144 152 169 174 179 182 256 266]\n",
      "train acc  0.9916593503072868  test acc  0.5588235294117647\n",
      "(68, 16)\n",
      "[ 0  1  2  3  4  5  7  9 10 11 12 13 14 15]\n",
      "train acc  0.9863915715539944  test acc  0.8088235294117647\n",
      "(68, 14)\n",
      "[ 0  3  4  5  6  7  8  9 10 11 12 13]\n",
      "train acc  0.9863915715539944  test acc  0.8235294117647058\n",
      "(68, 12)\n",
      "train acc phase1:  0.9863915715539944 test acc phase1:  0.7941176470588235 phase2:  0.7352941176470589\n"
     ]
    }
   ],
   "source": [
    "training_unimoal_results_depth = {}\n",
    "training_unimoal_results_leaf = {}\n",
    "for n_estimators in [5]:\n",
    "    training_unimoal_results_depth[n_estimators] = {}\n",
    "    training_unimoal_results_leaf[n_estimators] = {}\n",
    "    print(\"depth\")\n",
    "    for max_depth in [2,3,4]:\n",
    "        print(n_estimators,max_depth)\n",
    "        r = adaboost(X_phaseI_unimodal.fillna(NAFILL),X_phaseII_unimodal.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_depth=max_depth)\n",
    "        training_unimoal_results_depth[n_estimators][max_depth] = deepcopy(r)\n",
    "    print(\"leaf\")\n",
    "    for max_leaf in [3,4,5,6,7,8]:\n",
    "        print(n_estimators,max_leaf)\n",
    "        r = adaboost(X_phaseI_unimodal.fillna(NAFILL),X_phaseII_unimodal.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_leaf_nodes=max_leaf)\n",
    "        training_unimoal_results_leaf[n_estimators][max_leaf] = deepcopy(r)\n",
    "with open(focus_path + \"unimodal_train_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"depth\":training_unimoal_results_depth,\"leaf\":training_unimoal_results_leaf},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tricks from the beginning - Out of curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_full = [x for x in df_phaseI_select.columns.tolist()[5:] if (\"_allday\" not in x and \"_dis_\" not in x)]\n",
    "df_unimodal_full = df_phaseI_select[[\"PID\"] + cols_full]\n",
    "df_unimodal_full_mean = df_unimodal_full.groupby(\"PID\").mean()\n",
    "df_unimodal_full_mean.columns = [\"mean_\" + y for y in cols_full]\n",
    "df_unimodal_full_std = df_unimodal_full.groupby(\"PID\").std()\n",
    "df_unimodal_full_std.columns = [\"std_\" + y for y in cols_full]\n",
    "df_unimodal_full_mean_std = df_unimodal_full_mean.merge(df_unimodal_full_std, how = \"outer\", left_index=True, right_index=True)\n",
    "df_unimodal_full_mean_std = (df_unimodal_full_mean_std-df_unimodal_full_mean_std.mean())/df_unimodal_full_mean_std.std()\n",
    "X_phaseI_unimodal_full = deepcopy(df_unimodal_full_mean_std)\n",
    "X_phaseI_unimodal_full = X_phaseI_unimodal_full.loc[df_label.index]\n",
    "Y_phaseI = df_label[\"label_I\"]\n",
    "\n",
    "cols_full = [x for x in df_phaseII_select.columns.tolist()[5:] if (\"_allday\" not in x and \"_dis_\" not in x)]\n",
    "df_unimodal_full = df_phaseII_select[[\"PID\"] + cols_full]\n",
    "df_unimodal_full_mean = df_unimodal_full.groupby(\"PID\").mean()\n",
    "df_unimodal_full_mean.columns = [\"mean_\" + y for y in cols_full]\n",
    "df_unimodal_full_std = df_unimodal_full.groupby(\"PID\").std()\n",
    "df_unimodal_full_std.columns = [\"std_\" + y for y in cols_full]\n",
    "df_unimodal_full_mean_std = df_unimodal_full_mean.merge(df_unimodal_full_std, how = \"outer\", left_index=True, right_index=True)\n",
    "df_unimodal_full_mean_std = (df_unimodal_full_mean_std-df_unimodal_full_mean_std.mean())/df_unimodal_full_mean_std.std()\n",
    "X_phaseII_unimodal_full = deepcopy(df_unimodal_full_mean_std)\n",
    "X_phaseII_unimodal_full = X_phaseII_unimodal_full.loc[df_label.index]\n",
    "Y_phaseII = df_label[\"label_II\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n",
      "5 2\n",
      "(68, 1688)\n",
      "[ 435  736  855 1279]\n",
      "train acc  0.8878402107111499  test acc  0.7205882352941176\n",
      "(68, 4)\n",
      "train acc phase1:  0.8825724319578576 test acc phase1:  0.8382352941176471 phase2:  0.6029411764705882\n",
      "5 3\n",
      "(68, 1688)\n",
      "[ 125  212  435  504  640  736  839  855  879  890  926  965 1119 1279\n",
      " 1677 1684]\n",
      "train acc  0.9855136084284458  test acc  0.5882352941176471\n",
      "(68, 16)\n",
      "[ 1  2  3  5  6  7  8  9 10 12 13 14 15]\n",
      "train acc  0.9822212467076381  test acc  0.8235294117647058\n",
      "(68, 13)\n",
      "[ 0  1  2  3  5  6  7  8  9 10 12]\n",
      "train acc  0.9822212467076381  test acc  0.8088235294117647\n",
      "(68, 11)\n",
      "train acc phase1:  0.9822212467076381 test acc phase1:  0.8088235294117647 phase2:  0.4852941176470588\n",
      "5 4\n",
      "(68, 1688)\n",
      "[  17  284  736  855 1279 1398 1684]\n",
      "train acc  1.0  test acc  0.6176470588235294\n",
      "(68, 7)\n",
      "train acc phase1:  1.0 test acc phase1:  0.8235294117647058 phase2:  0.5441176470588235\n",
      "leaf\n",
      "5 3\n",
      "(68, 1688)\n",
      "[ 435  736 1279]\n",
      "train acc  0.8573309920983317  test acc  0.75\n",
      "(68, 3)\n",
      "train acc phase1:  0.8529411764705881 test acc phase1:  0.8382352941176471 phase2:  0.6176470588235294\n",
      "5 4\n",
      "(68, 1688)\n",
      "[ 435  736  879 1279 1684]\n",
      "train acc  0.9242756804214225  test acc  0.5735294117647058\n",
      "(68, 5)\n",
      "train acc phase1:  0.9233977172958737 test acc phase1:  0.8382352941176471 phase2:  0.4852941176470588\n",
      "5 5\n",
      "(68, 1688)\n",
      "[ 435  736  855  879 1279 1684]\n",
      "train acc  0.9556628621597895  test acc  0.5735294117647058\n",
      "(68, 6)\n",
      "train acc phase1:  0.9530289727831432 test acc phase1:  0.8382352941176471 phase2:  0.47058823529411764\n",
      "5 6\n",
      "(68, 1688)\n",
      "[  43  287  435  554  736  855 1279 1398 1684]\n",
      "train acc  0.9741000877963129  test acc  0.5882352941176471\n",
      "(68, 9)\n",
      "[1 3 4 5 6 7 8]\n",
      "train acc  0.9712467076382796  test acc  0.8529411764705882\n",
      "(68, 7)\n",
      "train acc phase1:  0.9712467076382796 test acc phase1:  0.8382352941176471 phase2:  0.5294117647058824\n",
      "5 7\n",
      "(68, 1688)\n",
      "[ 212  248  287  435  514  554  736  855 1092 1131 1279 1398 1649 1684]\n",
      "train acc  0.9879280070237046  test acc  0.6176470588235294\n",
      "(68, 14)\n",
      "[ 0  2  3  5  6  7  8  9 10 13]\n",
      "train acc  0.9852941176470587  test acc  0.8088235294117647\n",
      "(68, 10)\n",
      "train acc phase1:  0.9852941176470587 test acc phase1:  0.8382352941176471 phase2:  0.5\n",
      "5 8\n",
      "(68, 1688)\n",
      "[ 435  504  554  736  855 1534 1684]\n",
      "train acc  1.0  test acc  0.6470588235294118\n",
      "(68, 7)\n",
      "[0 1 2 3 4 6]\n",
      "train acc  1.0  test acc  0.8382352941176471\n",
      "(68, 6)\n",
      "train acc phase1:  1.0 test acc phase1:  0.8235294117647058 phase2:  0.5147058823529411\n"
     ]
    }
   ],
   "source": [
    "training_unimoal_full_results_depth = {}\n",
    "training_unimoal_full_results_leaf = {}\n",
    "for n_estimators in [5]:\n",
    "    training_unimoal_full_results_depth[n_estimators] = {}\n",
    "    training_unimoal_full_results_leaf[n_estimators] = {}\n",
    "    print(\"depth\")\n",
    "    for max_depth in [2,3,4]:\n",
    "        print(n_estimators,max_depth)\n",
    "        r = adaboost(X_phaseI_unimodal_full.fillna(NAFILL),X_phaseII_unimodal_full.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_depth=max_depth)\n",
    "        training_unimoal_full_results_depth[n_estimators][max_depth] = deepcopy(r)\n",
    "    print(\"leaf\")\n",
    "    for max_leaf in [3,4,5,6,7,8]:\n",
    "        print(n_estimators,max_leaf)\n",
    "        r = adaboost(X_phaseI_unimodal_full.fillna(NAFILL),X_phaseII_unimodal_full.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_leaf_nodes=max_leaf)\n",
    "        training_unimoal_full_results_leaf[n_estimators][max_leaf] = deepcopy(r)\n",
    "with open(focus_path + \"unimodal_full_train_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"depth\":training_unimoal_full_results_depth,\"leaf\":training_unimoal_full_results_leaf},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextually Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phaseI_contextual = pd.read_csv(focus_path + \"X_contextual_phaseI.csv\")\n",
    "X_phaseI_contextual = X_phaseI_contextual.set_index(\"PID\")\n",
    "Y_phaseI = df_label[\"label_I\"]\n",
    "X_phaseII_contextual = pd.read_csv(focus_path + \"X_contextual_phaseII.csv\")\n",
    "X_phaseII_contextual = X_phaseII_contextual.set_index(\"PID\")\n",
    "Y_phaseII = df_label[\"label_II\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n",
      "5 2\n",
      "(68, 316)\n",
      "[ 37  40  87 214 294]\n",
      "train acc  0.8037752414398593  test acc  0.5441176470588235\n",
      "(68, 5)\n",
      "train acc phase1:  0.7741439859525899 test acc phase1:  0.5882352941176471 phase2:  0.6323529411764706\n",
      "5 3\n",
      "(68, 316)\n",
      "[  3  21  23  37  40  87 127 185 206 268 294]\n",
      "train acc  0.9161545215100967  test acc  0.6029411764705882\n",
      "(68, 11)\n",
      "train acc phase1:  0.9078138718173837 test acc phase1:  0.7058823529411765 phase2:  0.6176470588235294\n",
      "5 4\n",
      "(68, 316)\n",
      "[ 21  23  84  85  87 116 121 149 162 185 268 294]\n",
      "train acc  0.9839771729587358  test acc  0.5294117647058824\n",
      "(68, 12)\n",
      "train acc phase1:  0.9890254609306408 test acc phase1:  0.7205882352941176 phase2:  0.5\n",
      "leaf\n",
      "5 3\n",
      "(68, 316)\n",
      "[ 37  40  87 294]\n",
      "train acc  0.7809482001755926  test acc  0.45588235294117646\n",
      "(68, 4)\n",
      "train acc phase1:  0.7640474100087797 test acc phase1:  0.5588235294117647 phase2:  0.6323529411764706\n",
      "5 4\n",
      "(68, 316)\n",
      "[ 27  40  87 125 185 268 294]\n",
      "train acc  0.8628182616330115  test acc  0.5882352941176471\n",
      "(68, 7)\n",
      "train acc phase1:  0.8478928884986829 test acc phase1:  0.7205882352941176 phase2:  0.6323529411764706\n",
      "5 5\n",
      "(68, 316)\n",
      "[ 87  95 106 116 162 176 185 268 294]\n",
      "train acc  0.9203248463564532  test acc  0.5147058823529411\n",
      "(68, 9)\n",
      "[0 2 4 6 7 8]\n",
      "train acc  0.9124231782265146  test acc  0.7205882352941176\n",
      "(68, 6)\n",
      "train acc phase1:  0.9122036874451274 test acc phase1:  0.7205882352941176 phase2:  0.5294117647058824\n",
      "5 6\n",
      "(68, 316)\n",
      "[ 84  85  87  95 106 121 149 162 176 185 268 294]\n",
      "train acc  0.9486391571553995  test acc  0.5147058823529411\n",
      "(68, 12)\n",
      "[ 0  1  2  3  4  6  7  8  9 10 11]\n",
      "train acc  0.9492976294995612  test acc  0.7205882352941176\n",
      "(68, 11)\n",
      "[ 0  1  2  3  4  5  6  8  9 10]\n",
      "train acc  0.9488586479367868  test acc  0.7205882352941176\n",
      "(68, 10)\n",
      "train acc phase1:  0.9492976294995612 test acc phase1:  0.7352941176470589 phase2:  0.5147058823529411\n",
      "5 7\n",
      "(68, 316)\n",
      "[ 23  84  85  87 106 111 116 149 162 185 268 294]\n",
      "train acc  0.9776119402985076  test acc  0.6029411764705882\n",
      "(68, 12)\n",
      "[ 0  1  2  3  4  5  7  8  9 10 11]\n",
      "train acc  0.9813432835820893  test acc  0.75\n",
      "(68, 11)\n",
      "train acc phase1:  0.9809043020193151 test acc phase1:  0.7352941176470589 phase2:  0.5147058823529411\n",
      "5 8\n",
      "(68, 316)\n",
      "[ 13  21  23  32  84  85  87 111 115 116 148 149 162 176 185 268 294]\n",
      "train acc  0.9907813871817381  test acc  0.5147058823529411\n",
      "(68, 17)\n",
      "[ 0  1  2  3  4  5  6  8  9 10 11 12 13 14 15 16]\n",
      "train acc  0.996488147497805  test acc  0.7205882352941176\n",
      "(68, 16)\n",
      "[ 0  1  4  5  6  7  8 10 11 12 13 14 15]\n",
      "train acc  0.9973661106233538  test acc  0.7205882352941176\n",
      "(68, 13)\n",
      "train acc phase1:  0.9969271290605793 test acc phase1:  0.7058823529411765 phase2:  0.5147058823529411\n"
     ]
    }
   ],
   "source": [
    "training_contextual_results_depth = {}\n",
    "training_contextual_results_leaf = {}\n",
    "for n_estimators in [5]:\n",
    "    training_contextual_results_depth[n_estimators] = {}\n",
    "    training_contextual_results_leaf[n_estimators] = {}\n",
    "    print(\"depth\")\n",
    "    for max_depth in [2,3,4]:\n",
    "        print(n_estimators,max_depth)\n",
    "        r = adaboost(X_phaseI_contextual.fillna(NAFILL),X_phaseII_contextual.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_depth=max_depth)\n",
    "        training_contextual_results_depth[n_estimators][max_depth] = deepcopy(r)\n",
    "    print(\"leaf\")\n",
    "    for max_leaf in [3,4,5,6,7,8]:\n",
    "        print(n_estimators,max_leaf)\n",
    "        r = adaboost(X_phaseI_contextual.fillna(NAFILL),X_phaseII_contextual.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_leaf_nodes=max_leaf)\n",
    "        training_contextual_results_leaf[n_estimators][max_leaf] = deepcopy(r)\n",
    "with open(focus_path + \"contextual_train_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"depth\":training_contextual_results_depth,\"leaf\":training_contextual_results_leaf},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phaseI_hybrid = X_phaseI_unimodal.merge(X_phaseI_contextual, left_index=True, right_index=True)\n",
    "Y_phaseI = df_label[\"label_I\"]\n",
    "X_phaseII_hybrid = X_phaseII_unimodal.merge(X_phaseII_contextual, left_index=True, right_index=True)\n",
    "Y_phaseII = df_label[\"label_II\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth\n",
      "5 2\n",
      "(68, 612)\n",
      "[126 142 159 174 493]\n",
      "train acc  0.8893766461808604  test acc  0.5\n",
      "(68, 5)\n",
      "train acc phase1:  0.8823529411764703 test acc phase1:  0.7647058823529411 phase2:  0.5588235294117647\n",
      "5 3\n",
      "(68, 612)\n",
      "[ 10  30  36 126 142 159 174 266 336 390 416 432 493 500 571]\n",
      "train acc  0.9804653204565409  test acc  0.6470588235294118\n",
      "(68, 15)\n",
      "[ 1  3  4  5  6  8  9 10 12 13 14]\n",
      "train acc  0.9734416154521509  test acc  0.7647058823529411\n",
      "(68, 11)\n",
      "train acc phase1:  0.9734416154521509 test acc phase1:  0.75 phase2:  0.6323529411764706\n",
      "5 4\n",
      "(68, 612)\n",
      "[104 126 141 159 493 500 570]\n",
      "train acc  0.9997805092186127  test acc  0.6029411764705882\n",
      "(68, 7)\n",
      "[1 2 3 4 5 6]\n",
      "train acc  1.0  test acc  0.8676470588235294\n",
      "(68, 6)\n",
      "train acc phase1:  1.0 test acc phase1:  0.8823529411764706 phase2:  0.6323529411764706\n",
      "leaf\n",
      "5 3\n",
      "(68, 612)\n",
      "[126 142 493]\n",
      "train acc  0.8454784899034239  test acc  0.5588235294117647\n",
      "(68, 3)\n",
      "train acc phase1:  0.838235294117647 test acc phase1:  0.7794117647058824 phase2:  0.5147058823529411\n",
      "5 4\n",
      "(68, 612)\n",
      "[126 142 336 390 400 406 493 500]\n",
      "train acc  0.9293239683933274  test acc  0.6470588235294118\n",
      "(68, 8)\n",
      "[0 1 2 3 4 6 7]\n",
      "train acc  0.9262510974539069  test acc  0.8235294117647058\n",
      "(68, 7)\n",
      "[0 1 2 4 5 6]\n",
      "train acc  0.9262510974539069  test acc  0.8235294117647058\n",
      "(68, 6)\n",
      "train acc phase1:  0.9262510974539069 test acc phase1:  0.8235294117647058 phase2:  0.5882352941176471\n",
      "5 5\n",
      "(68, 612)\n",
      "[126 134 142 159 174 234 336 406 410 493 500 571]\n",
      "train acc  0.9798068481123792  test acc  0.6323529411764706\n",
      "(68, 12)\n",
      "[ 0  2  3  4  5  6  7  8  9 10 11]\n",
      "train acc  0.9820017559262508  test acc  0.7941176470588235\n",
      "(68, 11)\n",
      "[ 0  1  2  3  4  5  7  8  9 10]\n",
      "train acc  0.9820017559262508  test acc  0.7941176470588235\n",
      "(68, 10)\n",
      "[0 2 3 4 5 6 7 8 9]\n",
      "train acc  0.9820017559262508  test acc  0.7941176470588235\n",
      "(68, 9)\n",
      "train acc phase1:  0.9820017559262508 test acc phase1:  0.7941176470588235 phase2:  0.6323529411764706\n",
      "5 6\n",
      "(68, 612)\n",
      "[126 142 159 174 234 336 406 410 486 493 500 571]\n",
      "train acc  0.9903424056189638  test acc  0.5882352941176471\n",
      "(68, 12)\n",
      "[ 0  1  2  3  4  5  6  7  9 10 11]\n",
      "train acc  0.9894644424934151  test acc  0.8235294117647058\n",
      "(68, 11)\n",
      "train acc phase1:  0.9894644424934151 test acc phase1:  0.8235294117647058 phase2:  0.6029411764705882\n",
      "5 7\n",
      "(68, 612)\n",
      "[ 11  36 111 126 142 155 159 174 234 336 406 416 433 487 493 500 570 571]\n",
      "train acc  0.9916593503072868  test acc  0.6176470588235294\n",
      "(68, 18)\n",
      "[ 2  3  4  6  7  8  9 10 12 13 14 15 16 17]\n",
      "train acc  0.9907813871817381  test acc  0.8235294117647058\n",
      "(68, 14)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 13]\n",
      "train acc  0.9907813871817381  test acc  0.8235294117647058\n",
      "(68, 12)\n",
      "train acc phase1:  0.9907813871817381 test acc phase1:  0.8382352941176471 phase2:  0.6029411764705882\n",
      "5 8\n",
      "(68, 612)\n",
      "[142 174 225 493 500 568 571]\n",
      "train acc  0.9997805092186127  test acc  0.6323529411764706\n",
      "(68, 7)\n",
      "train acc phase1:  1.0 test acc phase1:  0.8529411764705882 phase2:  0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "training_hybrid_results_depth = {}\n",
    "training_hybrid_results_leaf = {}\n",
    "for n_estimators in [5]:\n",
    "    training_hybrid_results_depth[n_estimators] = {}\n",
    "    training_hybrid_results_leaf[n_estimators] = {}\n",
    "    print(\"depth\")\n",
    "    for max_depth in [2,3,4]:\n",
    "        print(n_estimators,max_depth)\n",
    "        r = adaboost(X_phaseI_hybrid.fillna(NAFILL),X_phaseII_hybrid.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_depth=max_depth)\n",
    "        training_hybrid_results_depth[n_estimators][max_depth] = deepcopy(r)\n",
    "    print(\"leaf\")\n",
    "    for max_leaf in [3,4,5,6,7,8]:\n",
    "        print(n_estimators,max_leaf)\n",
    "        r = adaboost(X_phaseI_hybrid.fillna(NAFILL),X_phaseII_hybrid.fillna(NAFILL),Y_phaseI,Y_phaseII,n_estimators,max_leaf_nodes=max_leaf)\n",
    "        training_hybrid_results_leaf[n_estimators][max_leaf] = deepcopy(r)\n",
    "with open(focus_path + \"hybrid_train_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"depth\":training_hybrid_results_depth,\"leaf\":training_hybrid_results_leaf},f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8382352941176471"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57 / 68.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
