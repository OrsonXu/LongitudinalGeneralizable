{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "import subprocess\n",
    "\n",
    "NAFILL = -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config_general.json\", \"r\") as f:\n",
    "    config_general = json.load(f)\n",
    "folder_path = config_general[\"data_root_path\"]\n",
    "epis = config_general[\"epis\"]\n",
    "wks = config_general[\"wks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + \"raw_data_pool.pkl\", \"rb\") as f:\n",
    "    raw_data_pool = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_specific_filename = \"config_specific_cmu_two_non_overlap.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_focus': 'BDI_full', 'phaseI_name': 'data_phaseI_bdivalid', 'phaseII_name': 'data_phaseII_bdivalid', 'feature_col_epis_filename': 'feature_col_epis_cmu.json', 'feature_dis_col_epis_filename': 'feature_dis_col_epis_cmu.json', 'mutual_info_num_feature': 100, 'coorelation_filtering_threshold': 0.5, 'folder_path': 'C:/Users/orson/Desktop/Myself/HCI/UWiSchool/Projects/LongitudinalModeling/LongitudinalDataSet/Selection_Group', 'thresholds_for_rule_mining': [[[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]], [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]]}\n"
     ]
    }
   ],
   "source": [
    "with open(config_specific_filename, \"r\") as f:\n",
    "    config_specific = json.load(f)\n",
    "print(config_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_path = folder_path + \"Selection_Group/\" + config_specific[\"label_focus\"] + \"/\"\n",
    "if (not os.path.exists(focus_path)):\n",
    "    os.mkdir(focus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_phaseI_select = raw_data_pool[\"data_phaseI\"]\n",
    "# df_phaseII_select = raw_data_pool[\"data_phaseII\"]\n",
    "# df_label = raw_data_pool[\"labels\"]\n",
    "\n",
    "df_phaseI_data = raw_data_pool[config_specific[\"phaseI_name\"]][\"data\"]\n",
    "df_phaseI_label = raw_data_pool[config_specific[\"phaseI_name\"]][\"label\"]\n",
    "df_phaseII_data = raw_data_pool[config_specific[\"phaseII_name\"]][\"data\"]\n",
    "df_phaseII_label = raw_data_pool[config_specific[\"phaseII_name\"]][\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(focus_path + \"top_features_raw_dis.json\", \"r\") as f:\n",
    "    top_features_dis = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(folder_path + config_specific[\"feature_dis_col_epis_filename\"],\"r\") as f:\n",
    "    feature_dis_col_epis = json.load(f)\n",
    "with open(folder_path + \"symbol_to_int_map_day.json\", \"r\") as f:\n",
    "    symbol_to_int_map_day = json.load(f)\n",
    "with open(folder_path + \"int_to_symbol_map_day.json\", \"r\") as f:\n",
    "    int_to_symbol_map_day = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "PID_list_grp1 = df_phaseI_label[df_phaseI_label[\"label\"]][\"PID\"].tolist()\n",
    "PID_list_grp2 = df_phaseI_label[~df_phaseI_label[\"label\"]][\"PID\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.exists(focus_path + \"SPMF_input/\")):\n",
    "    os.mkdir(focus_path + \"SPMF_input/\")\n",
    "if (not os.path.exists(focus_path + \"SPMF_output/\")):\n",
    "    os.mkdir(focus_path + \"SPMF_output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wkdy mo\n",
      "(7764, 40)\n",
      "wkdy af\n",
      "(8466, 35)\n",
      "wkdy ev\n",
      "(8229, 47)\n",
      "wkdy ni\n",
      "(6313, 39)\n",
      "wkend mo\n",
      "(2708, 34)\n",
      "wkend af\n",
      "(3125, 30)\n",
      "wkend ev\n",
      "(3112, 32)\n",
      "wkend ni\n",
      "(2697, 34)\n"
     ]
    }
   ],
   "source": [
    "for wk in wks:\n",
    "    for epi in epis:\n",
    "        print(wk,epi)\n",
    "        df_buf = df_phaseI_data[df_phaseI_data[\"wk\"] == wk][[\"PID\"] + feature_dis_col_epis[epi]]\n",
    "        feature_cols = top_features_dis[wk][epi]\n",
    "        feature_cols = [x.replace(\"_\" + wk, \"\") for x in feature_cols]\n",
    "        df_selected = df_buf[[\"PID\"] + feature_cols]\n",
    "#         plt.hist(df_selected.isnull().sum(axis = 1))\n",
    "        df_selected = df_selected[df_selected.isnull().sum(axis = 1) <= df_selected.shape[1]/2]\n",
    "        for data_col in feature_cols:\n",
    "            df_selected[data_col] = df_selected[data_col].map(symbol_to_int_map_day[data_col])\n",
    "            \n",
    "        asso_input_grp1 = []\n",
    "        asso_input_grp2 = []\n",
    "        for index, line in df_selected[feature_cols][df_selected[\"PID\"].apply(lambda x : x in PID_list_grp1)].iterrows():\n",
    "            line = [str(int(float(x))) for x in line.dropna().tolist()]\n",
    "            asso_input_grp1.append(\" \".join(line))\n",
    "        for index, line in df_selected[feature_cols][df_selected[\"PID\"].apply(lambda x : x in PID_list_grp2)].iterrows():\n",
    "            line = [str(int(float(x))) for x in line.dropna().tolist()]\n",
    "            asso_input_grp2.append(\" \".join(line))\n",
    "        with open(focus_path + \"SPMF_input/\" + \"asso_\" + \"_\".join([wk,epi]) + \"_grp1.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(asso_input_grp1))\n",
    "        with open(focus_path + \"SPMF_input/\" + \"asso_\" + \"_\".join([wk,epi]) + \"_grp2.txt\", \"w\") as f:\n",
    "            f.write(\"\\n\".join(asso_input_grp2))\n",
    "        print(df_selected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SPMF (a Java Lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_args = [\n",
    "    os.getcwd() + \"\\config_general.json\",\n",
    "    os.getcwd() + \"\\\\\" + config_specific_filename\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java version 1.8.0\n",
    "command = \"java -jar rule_mining.jar \" + \" \".join(path_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wkdy,mo,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 218886\n",
      " Total time ~ 617 ms\n",
      "===================================================\n",
      "wkdy,mo,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 51412\n",
      " Total time ~ 114 ms\n",
      "===================================================\n",
      "wkdy,af,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 64028\n",
      " Total time ~ 131 ms\n",
      "===================================================\n",
      "wkdy,af,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 25100\n",
      " Total time ~ 116 ms\n",
      "===================================================\n",
      "wkdy,ev,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 40722\n",
      " Total time ~ 98 ms\n",
      "===================================================\n",
      "wkdy,ev,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 13894\n",
      " Total time ~ 38 ms\n",
      "===================================================\n",
      "wkdy,ni,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 15354\n",
      " Total time ~ 30 ms\n",
      "===================================================\n",
      "wkdy,ni,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 8102\n",
      " Total time ~ 20 ms\n",
      "===================================================\n",
      "wkend,mo,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 4000\n",
      " Total time ~ 14 ms\n",
      "===================================================\n",
      "wkend,mo,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 3336\n",
      " Total time ~ 11 ms\n",
      "===================================================\n",
      "wkend,af,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 98432\n",
      " Total time ~ 234 ms\n",
      "===================================================\n",
      "wkend,af,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 19376\n",
      " Total time ~ 45 ms\n",
      "===================================================\n",
      "wkend,ev,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 418\n",
      " Total time ~ 5 ms\n",
      "===================================================\n",
      "wkend,ev,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 310\n",
      " Total time ~ 7 ms\n",
      "===================================================\n",
      "wkend,ni,grp1\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 48\n",
      " Total time ~ 3 ms\n",
      "===================================================\n",
      "wkend,ni,grp2\n",
      "=============  ASSOCIATION RULE GENERATION v2.19- STATS =============\n",
      " Number of association rules generated : 40\n",
      " Total time ~ 4 ms\n",
      "===================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(subprocess.check_output(command).decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not os.path.exists(focus_path + \"SPMF_parse/\")):\n",
    "    os.mkdir(focus_path + \"SPMF_parse/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_asso_output(inputpath, outputpath, sep = \" \", temporal_slice = \"_day\", lift_valid = True,\n",
    "                     supp_th = 0, conf_th = 0, lift_th = 0):\n",
    "#     with open(\"symbol_to_int_map\" + temporal_slice + \".json\", \"r\") as f:\n",
    "#         symbol_to_int_map = json.load(f)\n",
    "#     with open(\"int_to_symbol_map_day\" + temporal_slice + \".json\", \"r\") as f:\n",
    "#         int_to_symbol_map_day = json.load(f)\n",
    "    asso_output = []\n",
    "    asso_parse = []\n",
    "    with open(inputpath, \"r\") as f:\n",
    "        asso_output = f.readlines()\n",
    "    asso_output = list(set(asso_output))\n",
    "    for idx, d in enumerate(asso_output):\n",
    "        d = d[:-1]\n",
    "        d_sp_pound = d.split(\"#\")\n",
    "        r = d_sp_pound[0].split(\"==>\")\n",
    "        supp = round(float(d_sp_pound[1][4:]),4)\n",
    "        conf = round(float(d_sp_pound[2][5:]),4)\n",
    "        lift = -1\n",
    "        if (lift_valid):\n",
    "            lift = round(float(d_sp_pound[3][5:]),4)\n",
    "        if not (supp > supp_th or conf > conf_th or lift > lift_th):\n",
    "            continue\n",
    "        X = list(filter(None, r[0].split(sep)))\n",
    "        X = list([str(i) for i in np.sort([int(x) for x in X])])\n",
    "        X_sym = [int_to_symbol_map_day[i] for i in X]\n",
    "        Y = list(filter(None, r[1].split(sep)))\n",
    "        Y = list([str(i) for i in np.sort([int(y) for y in Y])])\n",
    "        Y_sym = [int_to_symbol_map_day[i] for i in Y]\n",
    "        asso_parse.append([idx + 1, X, Y, X_sym, Y_sym, supp, conf, lift])\n",
    "    df_asso = pd.DataFrame(asso_parse, columns= [\"idx\", \"X\", \"Y\", \"X_sym\", \"Y_sym\", \"supp\", \"conf\", \"lift\"])\n",
    "#     df_asso[\"supxconf\"] = df_asso[\"sup\"] * df_asso[\"conf\"]\n",
    "    df_asso = df_asso.sort_values([\"supp\",\"conf\",\"lift\"],ascending=False)\n",
    "    df_asso.to_excel(outputpath, index = False)\n",
    "    return df_asso\n",
    "def parse_output_two_grp(input_folder, output_folder, filename, func_str = parse_asso_output,\n",
    "                             sep = \" \", temporal_slice = \"_day\", lift_valid = True,\n",
    "                             supp_th = 0, conf_th = 0, lift_th = 0):\n",
    "#     with open(\"symbol_to_int_map\" + temporal_slice + \".json\", \"r\") as f:\n",
    "#         symbol_to_int_map = json.load(f)\n",
    "#     with open(\"int_to_symbol_map_day\" + temporal_slice + \".json\", \"r\") as f:\n",
    "#         int_to_symbol_map_day = json.load(f)\n",
    "        \n",
    "    input_file_grp1 = input_folder + filename + \"_grp1.txt\"\n",
    "    input_file_grp2 = input_folder + filename + \"_grp2.txt\"\n",
    "    output_file_grp1 = output_folder + filename + \"_grp1.xlsx\"\n",
    "    output_file_grp2 = output_folder + filename + \"_grp2.xlsx\"\n",
    "    print(\"grp1 parse\")\n",
    "    df_grp1_buf = func_str(input_file_grp1, output_file_grp1, sep, temporal_slice, lift_valid,\n",
    "                                   supp_th, conf_th, lift_th)\n",
    "    print(\"grp2 parse\")\n",
    "    df_grp2_buf = func_str(input_file_grp2, output_file_grp2, sep, temporal_slice, lift_valid,\n",
    "                                   supp_th, conf_th, lift_th)\n",
    "    \n",
    "    print(\"grp 1 2 merge\")\n",
    "    df_grp1 = pd.read_excel(output_folder + filename + \"_grp1.xlsx\")\n",
    "    df_grp2 = pd.read_excel(output_folder + filename + \"_grp2.xlsx\")\n",
    "    df_comparison = pd.merge(df_grp1, df_grp2, on = [\"X\",\"Y\"], how = \"outer\")\n",
    "    df_comparison = df_comparison.drop([\"X_sym_x\", \"Y_sym_x\"], axis = 1)\n",
    "    df_comparison = df_comparison.drop([\"X_sym_y\", \"Y_sym_y\"], axis = 1)\n",
    "    df_comparison[\"X_sym\"] = pd.Series(list(map(lambda x : [int_to_symbol_map_day[i] for i in ast.literal_eval(x)], df_comparison[\"X\"])))\n",
    "    df_comparison[\"Y_sym\"] = pd.Series(list(map(lambda x : [int_to_symbol_map_day[i] for i in ast.literal_eval(x)], df_comparison[\"Y\"])))\n",
    "    df_comparison = df_comparison.fillna(0)\n",
    "    rename_cols = [\"idx\" + \"_grp1\"] + \\\n",
    "                [\"X\", \"Y\"] + \\\n",
    "                [x + \"_grp1\" for x in [\"supp\", \"conf\", \"lift\"]] + \\\n",
    "                [\"idx\" + \"_grp2\"] + \\\n",
    "                [x + \"_grp2\" for x in [\"supp\", \"conf\", \"lift\"]] + \\\n",
    "                [\"X_sym\", \"Y_sym\"]\n",
    "    reorder_cols = [\"X\", \"Y\"] +  [\"X_sym\", \"Y_sym\"] + \\\n",
    "                [\"idx\" + \"_grp1\"] + [x + \"_grp1\" for x in [\"supp\", \"conf\", \"lift\"]] + \\\n",
    "                [\"idx\" + \"_grp2\"] + [x + \"_grp2\" for x in [\"supp\", \"conf\", \"lift\"]]\n",
    "    df_comparison.columns = rename_cols\n",
    "    df_comparison= df_comparison[reorder_cols]\n",
    "    df_comparison[\"supp_diff\"] = np.abs(df_comparison[\"supp\" + \"_grp1\"] - df_comparison[\"supp\" + \"_grp2\"])\n",
    "    df_comparison[\"conf_diff\"] = np.abs(df_comparison[\"conf\" + \"_grp1\"] - df_comparison[\"conf\" + \"_grp2\"])\n",
    "    df_comparison[\"lift_diff\"] = np.abs(df_comparison[\"lift\" + \"_grp1\"] - df_comparison[\"lift\" + \"_grp2\"])\n",
    "#     df_comparison[\"supxconf_diff\"] = np.abs(df_comparison.supxconf_dep - df_comparison.supxconf_no_dep)\n",
    "    df_comparison = df_comparison.sort_values(by = [\"conf_diff\"], ascending=False)\n",
    "    df_comparison[\"supp_diff\"] = df_comparison[\"supp\" + \"_grp1\"] - df_comparison[\"supp\" + \"_grp2\"]\n",
    "    df_comparison[\"conf_diff\"] = df_comparison[\"conf\" + \"_grp1\"] - df_comparison[\"conf\" + \"_grp2\"]\n",
    "    df_comparison[\"lift_diff\"] = df_comparison[\"lift\" + \"_grp1\"] - df_comparison[\"lift\" + \"_grp2\"]\n",
    "#     df_comparison[\"supxconf_diff\"] = df_comparison.supxconf_dep - df_comparison.supxconf_no_dep\n",
    "    df_comparison.to_csv(output_folder + filename + \"_combine.csv\", index = False)\n",
    "    return df_grp1_buf, df_grp2_buf, df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wkdy mo\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkdy af\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkdy ev\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkdy ni\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkend mo\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkend af\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkend ev\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n",
      "wkend ni\n",
      "grp1 parse\n",
      "grp2 parse\n",
      "grp 1 2 merge\n"
     ]
    }
   ],
   "source": [
    "for wk in wks:\n",
    "    for epi in epis:\n",
    "        print(wk, epi)\n",
    "        input_folder = focus_path + \"SPMF_output/\"\n",
    "        output_folder = focus_path + \"SPMF_parse/\"\n",
    "        filename = \"asso_\" + wk + \"_\" + epi\n",
    "        df_asso_dep, df_asso_no_dep, df_comparison = parse_output_two_grp(\n",
    "            input_folder,\n",
    "            output_folder,\n",
    "            filename,\n",
    "            parse_asso_output\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
